{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b15cf-3f99-47f5-89c4-404c1fcbfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "  \n",
    "### Ridge Regression\n",
    "\n",
    "**Definition:**\n",
    "Ridge regression, also known as Tikhonov regularization, is a type of linear regression that includes a regularization term to the loss function. This regularization term helps to prevent overfitting by penalizing large coefficients, thus ensuring a more generalized model.\n",
    "\n",
    "**Equation:**\n",
    "The ridge regression model modifies the ordinary least squares (OLS) loss function by adding a penalty term proportional to the sum of the squared coefficients:\n",
    "\n",
    "\\[ \\text{Loss} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2} - \\ldots - \\beta_p x_{ip})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\) is the observed value.\n",
    "- \\( x_{ij} \\) are the independent variables.\n",
    "- \\( \\beta_j \\) are the coefficients.\n",
    "- \\( \\lambda \\) is the regularization parameter, controlling the strength of the penalty.\n",
    "\n",
    "### Differences from Ordinary Least Squares (OLS) Regression\n",
    "\n",
    "1. **Loss Function:**\n",
    "   - **OLS Regression:** Minimizes the sum of the squared residuals (errors).\n",
    "     \\[ \\text{Loss}_{OLS} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2} - \\ldots - \\beta_p x_{ip})^2 \\]\n",
    "   - **Ridge Regression:** Minimizes the sum of the squared residuals plus a penalty term proportional to the sum of the squared coefficients.\n",
    "     \\[ \\text{Loss}_{Ridge} = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_{i1} - \\beta_2 x_{i2} - \\ldots - \\beta_p x_{ip})^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\]\n",
    "\n",
    "2. **Regularization:**\n",
    "   - **OLS Regression:** No regularization is applied; it purely minimizes the residual sum of squares.\n",
    "   - **Ridge Regression:** Introduces regularization to penalize large coefficients, which helps to prevent overfitting and reduces model complexity.\n",
    "\n",
    "3. **Handling Multicollinearity:**\n",
    "   - **OLS Regression:** Can suffer from multicollinearity, where highly correlated independent variables can lead to unstable and high-variance coefficient estimates.\n",
    "   - **Ridge Regression:** Alleviates the problem of multicollinearity by shrinking the coefficients of correlated predictors, thus stabilizing the estimates.\n",
    "\n",
    "4. **Bias-Variance Tradeoff:**\n",
    "   - **OLS Regression:** Typically has lower bias but higher variance, especially when dealing with multicollinearity or high-dimensional data.\n",
    "   - **Ridge Regression:** Introduces some bias (through regularization) but reduces variance, often leading to a better overall predictive performance on new data.\n",
    "\n",
    "### Choosing the Regularization Parameter (\\(\\lambda\\))\n",
    "\n",
    "- The regularization parameter \\(\\lambda\\) controls the tradeoff between fitting the data well and keeping the coefficients small. \n",
    "- A \\(\\lambda\\) of 0 results in OLS regression.\n",
    "- As \\(\\lambda\\) increases, the impact of the regularization term increases, leading to smaller coefficients and potentially less overfitting.\n",
    "\n",
    "**Cross-Validation:** A common method to choose \\(\\lambda\\) is cross-validation, where the data is split into training and validation sets multiple times, and the \\(\\lambda\\) that minimizes the validation error is selected.\n",
    "\n",
    "### Example: Ridge Regression in Python\n",
    "\n",
    "Here's an example of how to implement ridge regression using Python's `scikit-learn` library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ridge regression model\n",
    "ridge_reg = Ridge(alpha=1)  # alpha is the regularization parameter λ\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "y_pred = ridge_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X_test, y_pred, color='red', label='Ridge Fit')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Ridge Regression:**\n",
    "  - Adds a regularization term to the loss function to penalize large coefficients.\n",
    "  - Helps prevent overfitting and handles multicollinearity.\n",
    "  - Includes a regularization parameter (\\(\\lambda\\)) that controls the strength of the penalty.\n",
    "\n",
    "- **Ordinary Least Squares (OLS) Regression:**\n",
    "  - Minimizes the residual sum of squares without any regularization.\n",
    "  - Can suffer from overfitting and multicollinearity in high-dimensional data.\n",
    "\n",
    "- **Use Cases:**\n",
    "  - **Ridge Regression:** Preferred when dealing with multicollinearity, high-dimensional data, or when the model tends to overfit.\n",
    "  - **OLS Regression:** Suitable for simple linear models with low-dimensional data and no multicollinearity issues.\n",
    "\n",
    "By understanding these differences and the role of regularization, one can choose the appropriate regression technique for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b6ecce-325e-46b2-8def-e27ba93c55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "   \n",
    "The assumptions are the same as those used in regular multiple regression: linearity, constant variance \n",
    "(no outliers), and independence. Since ridge regression does not provide confidence limits, normality need not\n",
    "be assumed. Ridge regression remains controversial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f06c0-eb95-4c31-bf1c-4e0102fa1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "Instead of arbitrarily choosing λ=4, it would be better to use cross-validation to choose the tuning parameter λ. \n",
    "We can do this using the built-in cross-validation function, cv. glmnet() . By default, the function performs \n",
    "10-fold cross-validation, though this can be changed using the argument folds ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe9ef2-f7ee-452a-b849-4a7ad9499963",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "   \n",
    "You need to stay within the shrinkage model context. You could see ridge regression as doing the feature \n",
    "'selection' in a nuanced way by reducing the size of the coefficients instead of setting them equal to zero. \n",
    "You could elliminate the features with the smaller coefficients*, but it is a bit crude method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c2254-9e88-47ec-8f9e-cc5f1692fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 5\n",
    "   \n",
    "Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of \n",
    "the regression coefficients, deflate the partial t-tests for the regression coefficients, give false, \n",
    "nonsignificant, p-values, and degrade the predictability of the model (and that's just for starters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd390f1a-dfc9-48ec-bcd9-d7e092d2175b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "  #Answer: 6\n",
    "   \n",
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, categorical variables need to be properly encoded before they can be used in the regression model. Here’s how you can handle both types of variables in Ridge Regression:\n",
    "\n",
    "### Continuous Variables\n",
    "Continuous variables can be directly used in the regression model. These are numerical variables that can take an infinite number of values within a given range.\n",
    "\n",
    "### Categorical Variables\n",
    "Categorical variables need to be transformed into a numerical format before they can be used in a regression model. There are several methods to encode categorical variables:\n",
    "\n",
    "1. **One-Hot Encoding:**\n",
    "   - Converts categorical variables into a series of binary (0 or 1) columns. Each unique category becomes a new column.\n",
    "   - This is the most common method for encoding categorical variables in linear regression models, including Ridge Regression.\n",
    "\n",
    "2. **Label Encoding:**\n",
    "   - Assigns a unique integer to each category. \n",
    "   - This method should be used with caution, as it introduces an ordinal relationship between categories that might not be appropriate.\n",
    "\n",
    "3. **Dummy Variables:**\n",
    "   - Similar to one-hot encoding but typically drops one category to avoid multicollinearity (the \"dummy variable trap\").\n",
    "\n",
    "### Example: Ridge Regression with Both Categorical and Continuous Variables in Python\n",
    "\n",
    "Here’s an example demonstrating how to handle both categorical and continuous variables in Ridge Regression using Python’s `scikit-learn` library:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'age': [25, 45, 35, 50],\n",
    "    'salary': [50000, 100000, 75000, 120000],\n",
    "    'city': ['New York', 'Los Angeles', 'New York', 'Chicago']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Dependent variable\n",
    "y = np.array([200, 400, 300, 500])\n",
    "\n",
    "# Independent variables\n",
    "X = df[['age', 'salary', 'city']]\n",
    "\n",
    "# Preprocessing pipeline\n",
    "# Standardize continuous variables and one-hot encode categorical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['age', 'salary']),\n",
    "        ('cat', OneHotEncoder(), ['city'])\n",
    "    ])\n",
    "\n",
    "# Ridge regression pipeline\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=1))\n",
    "])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "ridge_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = ridge_pipeline.predict(X_test)\n",
    "\n",
    "# Display results\n",
    "print(\"Predicted values:\", y_pred)\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - A sample dataset is created with both continuous (age, salary) and categorical (city) variables.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - `ColumnTransformer` is used to apply different preprocessing steps to different columns:\n",
    "     - `StandardScaler` standardizes the continuous variables (age, salary).\n",
    "     - `OneHotEncoder` encodes the categorical variable (city).\n",
    "\n",
    "3. **Pipeline:**\n",
    "   - The preprocessing steps and the `Ridge` regression model are combined into a single `Pipeline`. This ensures that the preprocessing is applied consistently during both training and prediction.\n",
    "\n",
    "4. **Model Training and Prediction:**\n",
    "   - The data is split into training and test sets.\n",
    "   - The model is fitted on the training data and used to make predictions on the test data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Ridge Regression can handle both categorical and continuous independent variables.\n",
    "- Categorical variables must be encoded into numerical format, typically using methods like one-hot encoding.\n",
    "- Combining preprocessing steps and the regression model into a pipeline ensures consistent and efficient handling of the data.\n",
    "\n",
    "By properly encoding categorical variables and using appropriate preprocessing techniques, Ridge Regression can effectively be applied to datasets with both categorical and continuous independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d273058-794f-4018-b3bc-eebc88035016",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 7\n",
    "   \n",
    "Adding a positive value k to the diagonal elements of X'X will break up any dependency between these columns. \n",
    "This will also cause the estimated regression coefficients to shrink toward the null; the higher the value of k,\n",
    "the greater the shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d90060-ed40-4807-ab1a-1f470899d67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 8\n",
    "   \n",
    "The ridge regression technique can be used to predict time-series. \n",
    "Ridge regression (RR) can also solve the multicollinearity problem that exists in linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
